[program started on Tue Jan 26 10:49:25 2016] 
[command line arguments] 
plot true 
preTrain false 
loss reinforce 
preTrainEpochs 50 
save testing 
momentum 0.9 
threads 1 
dataset mnist 
uniform 0.1 
seed 123 
weightDecay 0 
epochs 10000 
batchSize 20 
learningRate 0.01 
[----------------------] 
==> Loading scripts 
==> Loading data 
==> Preprocessing data 
==> Preprocessing normalization 
(1,.,.) = 
 Columns 1 to 9
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4113
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241  0.6960
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241  1.8924
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241  1.9306
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241  1.3451
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.3095
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241

Columns 10 to 18
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.3477  2.1851  2.3761  2.4015
 -0.4241 -0.4241 -0.4241 -0.4241 -0.3350  1.9433  2.8088  2.1088  1.1669
 -0.4241 -0.4241 -0.4241 -0.1950  1.7906  2.8088  0.9887 -0.3732 -0.4241
 -0.4241 -0.4241 -0.4241  0.0723  2.3633  0.8360 -0.3604 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241  0.5051
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241  1.2051
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.1822  2.4270
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241  0.6451  2.6561
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241  1.3069  1.4724
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241  0.9505  2.1088 -0.0677
 -0.4113  0.3014  1.3324  1.5615  0.7978  0.7978  2.5924  0.6196 -0.4241
  1.1542  2.7324  1.8160  1.8033  2.1979  2.8088  2.6943 -0.1695 -0.4241
  2.7579  0.1105 -0.4241 -0.1186  2.1088  2.7070  2.6688  1.5488 -0.4241
  1.6888 -0.4241 -0.2586  2.0579  2.4143 -0.0041  1.2178  2.7961 -0.0295
  1.4215 -0.2586  2.0706  2.5670  0.4414 -0.4241 -0.2332  2.8088  1.2051
  2.7324  2.2488  2.7070  0.5305 -0.4241 -0.4241 -0.3604  2.0070  2.5161
  0.8360  1.3833  0.0723 -0.4241 -0.4241 -0.4241 -0.4241  0.7723  2.8088
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241  0.4669  2.8215
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241  0.4669  2.8088
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241  0.4669  2.6179
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241

Columns 19 to 27
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
  1.2306 -0.3732 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
  2.4015  1.3960 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
  1.8924  1.6888 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
  2.4270  1.4597 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
  2.8088  0.4796 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
  2.6179 -0.2713 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
  0.9251 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.1313 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.3604 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.3477 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.1186 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.3477 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.3604 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241

Columns 28 to 28
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
[torch.DoubleTensor of size 1x28x28]
 
(1,.,.) = 
 Columns 1 to 9
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241  0.9124
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241  0.7851  2.8088
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241  0.9378  2.7961
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.0041  2.3379  2.7961
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.3859  1.6888  2.7961  2.7961
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.2968  2.4652  2.7961  2.7961
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.2968  2.4779  2.7961  2.7961
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.2586  2.7961  2.7961  2.7961
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.3986  1.0396  2.7961  2.7961
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.3859  1.6888  2.7961
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.3350  0.0214
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241

Columns 10 to 18
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.3223
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.2586  0.7851  2.2742
 -0.4241 -0.4241 -0.4241 -0.4241 -0.2204  0.7851  2.2361  2.7961  2.7961
 -0.4241 -0.4241  0.0850  0.7978  2.1979  2.7961  2.8088  2.7961  2.7961
  0.1360  1.8924  2.6306  2.7961  2.7961  2.7961  2.8088  2.7961  2.0961
 -0.2332  0.3396  0.3396  1.7142  2.7961  2.7961  2.8088  2.1215 -0.1313
 -0.4241 -0.4241  0.4669  2.7197  2.7961  2.7961  2.6943 -0.1568 -0.4241
 -0.4241  0.5305  2.2106  2.7961  2.7961  2.2106  0.7469 -0.4241 -0.4241
  0.5814  2.3633  2.7961  2.7961  2.7961  1.3324 -0.4241 -0.4241 -0.4241
  2.7579  2.7961  2.7961  2.7961  0.0087 -0.4113 -0.4241 -0.4241 -0.4241
  2.8088  2.8088  2.8088  0.7723 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
  2.7961  2.7961  2.1724 -0.2332 -0.4241 -0.4241 -0.4241 -0.4241 -0.1568
  2.7961  1.2560  0.3905 -0.4241 -0.4241 -0.1950  0.1232  1.5742  1.7524
  2.6561  0.1996 -0.2077  0.1996  1.5869  2.2488  2.8088  2.7961  2.7961
  2.2106 -0.2332  1.7651  2.7961  2.7961  2.7961  2.8088  2.7197  2.1342
  2.4270  0.6833  2.6561  2.7961  2.7961  1.9179  0.3396  0.2632 -0.3095
  2.7961  2.5161  2.7961  2.7961  2.7961  0.7596  0.6705  0.6705  0.6705
  2.7961  2.7961  2.7961  2.7961  2.7961  2.7961  2.8088  2.7961  2.7961
  2.7961  2.7961  2.7961  2.7961  2.7961  2.7961  2.8088  2.7961  2.7961
  1.2560  2.4397  2.7961  2.7961  2.7961  2.0579  1.2560  1.2560  1.2560
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241

Columns 19 to 27
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
  1.0651  2.8088  2.3761  0.7087 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
  2.7961  2.7961  2.7961  1.5742 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
  2.7961  2.6943  1.1669 -0.1950 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
  2.0961  0.3905 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.1186 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.3859 -0.2586 -0.2586 -0.2586 -0.3223 -0.4241 -0.4241 -0.4241 -0.4241
  1.6888  2.7961  2.7961  2.7961  2.2742 -0.1059 -0.4241 -0.4241 -0.4241
  2.7961  2.7961  2.7961  2.7961  2.7961  1.6124 -0.3986 -0.4241 -0.4241
  2.7961  2.7961  2.7961  2.7961  2.7961  2.7961 -0.2841 -0.4241 -0.4241
  2.7961  2.2488  2.2488  2.7961  2.7961  1.8033 -0.3732 -0.4241 -0.4241
  0.3396  0.0214  1.8924  2.7961  2.7961  0.2378 -0.4241 -0.4241 -0.4241
  0.9633  2.3379  2.7961  2.7961  1.2815 -0.3604 -0.4241 -0.4241 -0.4241
  2.7961  2.7961  2.7961  1.2815 -0.3604 -0.4241 -0.4241 -0.4241 -0.4241
  2.7961  1.8033  0.2378 -0.3604 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
  0.9760 -0.3732 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241
 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241 -0.4241

Columns 28 to 28
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
 -0.4241
[torch.DoubleTensor of size 1x28x28]
 
==> define parameters of the model 
==> construct model 
==> define loss 
==> here is the loss function: 
nn.ParallelCriterion 
==> defining some tools 
==> configuring optimizer 
==> defining training procedure 
==> defining test procedure 
==> Training 
==> doing epoch on training data: 
==> online epoch # 0 [batchSize = 20] 
2 
 -2.5539
 -7.4948
 -6.0885
 -7.5166
 -0.1103
-13.0675
 -3.7696
-11.6525
 -9.2451
-14.1924
[torch.DoubleTensor of size 10]
 
-0.46428571428571 X 0.46428571428571 
0.30764737581837 X -0.46428571428571 
0.46428571428571 X 0.35424010791047 
0.46428571428571 X 0.4294284017769 
0.41922881356276 X 0.36053851293636 
0.46428571428571 X -0.22129361743745 
0.46428571428571 X -0.37245154981764 

==> time to learn 1 sample = 9.0259882171949ms 
ConfusionMatrix:
[[    5392     156      91      71      37      82     465     334      84      30]   79.976% 	[class: 1]
 [     265    3457     216     169     114     373     180     732      99     353]   58.023% 	[class: 2]
 [     189     256    3765     148     784      60     254     153     336     186]   61.409% 	[class: 3]
 [     185     123      70    3930      71     233     194      57     922      57]   67.271% 	[class: 4]
 [      96     182     930     172    3261     177     103     115     255     130]   60.155% 	[class: 5]
 [      99     272      19     158      93    4655      34     160     109     319]   78.658% 	[class: 6]
 [     541     137     142     234      53      33    4407     159     528      31]   70.343% 	[class: 7]
 [     548     586     106     111      92     220     232    3344     231     381]   57.153% 	[class: 8]
 [     179      65     196     855     160     102     799      96    3405      92]   57.237% 	[class: 9]
 [      20     256     112      41      78     327      23     325      47    4694]]  79.250% 	[class: 0]
 + average row correct: 66.947577595711% 
 + average rowUcol correct (VOC measure): 50.634876489639% 
 + global correct: 67.183333333333% 
==> testing on test set: 
6 
-7.2556
-4.2321
-3.1742
-2.4538
-5.1301
-0.3118
-6.9356
-4.5802
-4.8500
-2.3053
[torch.DoubleTensor of size 10]
 
-0.46428571428571 X 0.46428571428571 
0.46428571428571 X 0.46428571428571 
0.46428571428571 X 0.46428571428571 
0.46428571428571 X 0.2922500595667 
0.40546591586184 X 0.42510206392862 
0.46428571428571 X -0.1244025903706 
0.46428571428571 X 0.45951866306692 

==> time to test 1 sample = 3.8534276008606ms 
ConfusionMatrix:
[[     557      62      84      61      75      12     183       3      75      23]   49.075% 	[class: 1]
 [     102     210     162      95      74     307      40       1      13      28]   20.349% 	[class: 2]
 [     101      87     532      32      77      76      51       1      30      23]   52.673% 	[class: 3]
 [      68      96      41     312      28     289      88       4      42      14]   31.772% 	[class: 4]
 [     171      80     196      64     129     141      70       4      17      20]   14.462% 	[class: 5]
 [     101     100      34      96      28     515      21       0      30      33]   53.758% 	[class: 6]
 [     184     187     158      94      56      44     236       3      57       9]   22.957% 	[class: 7]
 [     172     156     187     109     101      70     101       7      40      31]   0.719% 	[class: 8]
 [     104      99      98     275      52     148     147       4      60      22]   5.946% 	[class: 9]
 [      38     207      95     108      55     360      26       1      16      74]]  7.551% 	[class: 0]
 + average row correct: 25.926198982634% 
 + average rowUcol correct (VOC measure): 13.292085477151% 
 + global correct: 26.32% 
==> doing epoch on training data: 
==> online epoch # 1 [batchSize = 20] 
2 
-1.3260e+02
-2.6059e+01
-1.1350e+01
-1.3279e-05
-3.8933e+01
-6.6417e+01
-1.0296e+02
-4.9634e+01
-2.9695e+01
-2.0136e+01
[torch.DoubleTensor of size 10]
 
-0.44382274915944 X 0.45262631846121 
0.45043799943615 X -0.41561438420362 
0.46428571428571 X -0.46428571428571 
0.46428571428571 X 0.46428571428571 
0.46428571428571 X 0.46428571428571 
0.4155028024137 X 0.30586268123146 
0.46428571428571 X 0.447535211477 

==> time to learn 1 sample = 9.6593298316002ms 
ConfusionMatrix:
[[    1333     594    1727    1476      22      41     494     600      30     425]   19.772% 	[class: 1]
 [     127    2268     563    2403       2      13      65     125       0     392]   38.066% 	[class: 2]
 [     146     816     507    3677       3       3      47     153       0     779]   8.269% 	[class: 3]
 [      63    3886     217    1516       1       8      43      43       0      65]   25.950% 	[class: 4]
 [      93     815    1214    1389       2      22      69     589       0    1228]   0.037% 	[class: 5]
 [     850    1787     419    1812       0      35      57     173       1     784]   0.591% 	[class: 6]
 [      33     853     335    4866       1       2      39      38       1      97]   0.623% 	[class: 7]
 [      28    1747     654    2683       3      15      91     228       2     400]   3.897% 	[class: 8]
 [      45    2751     341    2544       0       8      65      64       0     131]   0.000% 	[class: 9]
 [       0    2808     381    1845       0       9      53     128       0     699]]  11.801% 	[class: 0]
 + average row correct: 10.900655031146% 
 + average rowUcol correct (VOC measure): 4.7175665423856% 
 + global correct: 11.045% 
==> testing on test set: 
6 
-15.4668
 -4.4556
 -5.0360
 -0.0597
-10.2622
 -4.1449
 -5.8679
 -5.4742
 -6.8235
 -4.1577
[torch.DoubleTensor of size 10]
 
-0.46428571428571 X 0.42002015051501 
0.46428571428571 X 0.44579004401563 
0.31911853840632 X 0.2445921100569 
0.46428571428571 X 0.46428571428571 
0.35016850638381 X 0.46428571428571 
0.40316264779368 X -0.46428571428571 
0.46428571428571 X 0.46428571428571 

==> time to test 1 sample = 4.0858920097351ms 
ConfusionMatrix:
[[     229      97     284     263       8       7      86      84       4      73]   20.176% 	[class: 1]
 [      18     385      96     426       1       8       9      18       0      71]   37.306% 	[class: 2]
 [      22     134      80     617       0       0       9      21       0     127]   7.921% 	[class: 3]
 [      13     636      18     286       0       0       4       7       0      18]   29.124% 	[class: 4]
 [      13     134     212     226       0       3      11      86       0     207]   0.000% 	[class: 5]
 [     160     328      53     275       0       4       5      19       0     114]   0.418% 	[class: 6]
 [       9     122      55     816       0       0       8       7       0      11]   0.778% 	[class: 7]
 [       4     294     105     465       1       1      15      34       0      55]   3.491% 	[class: 8]
 [       6     476      53     430       1       0      10       4       0      29]   0.000% 	[class: 9]
 [       0     492      49     293       0       4      11      17       0     114]]  11.633% 	[class: 0]
 + average row correct: 11.084660030901% 
 + average rowUcol correct (VOC measure): 4.7571290517226% 
 + global correct: 11.4% 
==> doing epoch on training data: 
==> online epoch # 2 [batchSize = 20] 
